** Config **
adapter: False
adapter_dim: None
adaptformer: True
backbone: CLIP-ViT-B/16
batch_size: 128
bias_tuning: False
bn_tuning: False
classifier: CosineClassifier
dataset: KneeOA
deterministic: True
expand: 24
full_tuning: False
gpu: 0
imb_factor: None
init_head: text_feat
ln_tuning: False
lora: False
loss_type: LA
lr: 0.01
micro_batch_size: 128
model_dir: None
momentum: 0.9
num_epochs: 10
num_workers: 8
output_dir: ./output/kneeoa_clip_vit_b16_peft
partial: None
prec: amp
print_freq: 10
resolution: 224
root: /home/padma/DH-602_project/datasets/kneeOA/KneeXrayData/ClsKLData/kneeKL224
scale: 25
seed: 0
ssf_attn: False
ssf_ln: False
ssf_mlp: False
test_ensemble: True
test_only: False
test_train: False
vpt_deep: False
vpt_len: None
vpt_shallow: False
weight_decay: 0.0005
zero_shot: False
************
Setting fixed seed: 0
mean: [0.48145466, 0.4578275, 0.40821073]
std: [0.26862954, 0.26130258, 0.27577711]
Total training points: 5778
Building model
Loading CLIP (backbone: CLIP-ViT-B/16)
Adapter bottle dimension set to 1
Turning off gradients in the model
Turning on gradients in the tuner
Turning on gradients in the head
Total params: 149670680
Tuned params: 46104
Head params: 3840
Initialize head with text features
Initialize tensorboard (log_dir=./output/kneeoa_clip_vit_b16_peft/tensorboard)
epoch [1/10] batch [10/46] time 0.297 (0.757) data 0.000 (0.419) loss 1.5555 (1.4091) acc 20.3125 (32.4957) (mean 20.8352 many 20.8352 med nan few nan) lr 1.0000e-02 eta 0:05:40
epoch [1/10] batch [20/46] time 0.302 (0.533) data 0.000 (0.210) loss 1.5562 (1.4701) acc 28.1250 (27.9112) (mean 22.0984 many 22.0984 med nan few nan) lr 1.0000e-02 eta 0:03:54
epoch [1/10] batch [30/46] time 0.301 (0.456) data 0.001 (0.140) loss 1.4630 (1.4997) acc 39.0625 (24.4360) (mean 27.2639 many 27.2639 med nan few nan) lr 1.0000e-02 eta 0:03:16
epoch [1/10] batch [40/46] time 0.300 (0.417) data 0.000 (0.105) loss 1.3917 (1.4082) acc 41.4062 (31.7929) (mean 29.2419 many 29.2419 med nan few nan) lr 1.0000e-02 eta 0:02:55
epoch [2/10] batch [10/46] time 0.305 (0.426) data 0.000 (0.121) loss 1.2608 (1.3117) acc 27.3438 (31.9959) (mean 26.3402 many 26.3402 med nan few nan) lr 9.7553e-03 eta 0:02:52
epoch [2/10] batch [20/46] time 0.303 (0.408) data 0.000 (0.103) loss 1.3106 (1.3179) acc 25.0000 (34.2486) (mean 27.4791 many 27.4791 med nan few nan) lr 9.7553e-03 eta 0:02:40
epoch [2/10] batch [30/46] time 0.302 (0.394) data 0.001 (0.090) loss 1.2965 (1.3007) acc 34.3750 (33.0663) (mean 32.2231 many 32.2231 med nan few nan) lr 9.7553e-03 eta 0:02:31
epoch [2/10] batch [40/46] time 0.301 (0.383) data 0.000 (0.079) loss 1.1659 (1.2690) acc 42.9688 (35.2913) (mean 41.1892 many 41.1892 med nan few nan) lr 9.7553e-03 eta 0:02:23
epoch [3/10] batch [10/46] time 0.303 (0.394) data 0.000 (0.093) loss 1.1548 (1.2305) acc 46.0938 (37.5362) (mean 36.1505 many 36.1505 med nan few nan) lr 9.0451e-03 eta 0:02:20
epoch [3/10] batch [20/46] time 0.304 (0.386) data 0.000 (0.084) loss 1.2351 (1.2316) acc 46.8750 (38.3711) (mean 45.5717 many 45.5717 med nan few nan) lr 9.0451e-03 eta 0:02:14
epoch [3/10] batch [30/46] time 0.304 (0.379) data 0.001 (0.077) loss 1.1340 (1.2108) acc 46.0938 (40.0056) (mean 38.2029 many 38.2029 med nan few nan) lr 9.0451e-03 eta 0:02:08
epoch [3/10] batch [40/46] time 0.302 (0.373) data 0.000 (0.072) loss 1.2198 (1.2284) acc 44.5312 (38.2735) (mean 42.4466 many 42.4466 med nan few nan) lr 9.0451e-03 eta 0:02:02
epoch [4/10] batch [10/46] time 0.305 (0.382) data 0.000 (0.083) loss 1.0298 (1.1825) acc 46.8750 (39.3994) (mean 44.2844 many 44.2844 med nan few nan) lr 7.9389e-03 eta 0:01:59
epoch [4/10] batch [20/46] time 0.304 (0.378) data 0.000 (0.077) loss 1.1319 (1.1916) acc 44.5312 (38.6682) (mean 48.3679 many 48.3679 med nan few nan) lr 7.9389e-03 eta 0:01:54
epoch [4/10] batch [30/46] time 0.304 (0.373) data 0.000 (0.073) loss 1.0840 (1.1627) acc 45.3125 (38.5309) (mean 44.3240 many 44.3240 med nan few nan) lr 7.9389e-03 eta 0:01:48
epoch [4/10] batch [40/46] time 0.303 (0.369) data 0.000 (0.069) loss 1.1137 (1.1387) acc 47.6562 (44.0238) (mean 48.9032 many 48.9032 med nan few nan) lr 7.9389e-03 eta 0:01:44
epoch [5/10] batch [10/46] time 0.305 (0.376) data 0.000 (0.076) loss 1.1641 (1.1127) acc 40.6250 (46.0434) (mean 40.4982 many 40.4982 med nan few nan) lr 6.5451e-03 eta 0:01:39
epoch [5/10] batch [20/46] time 0.305 (0.372) data 0.000 (0.072) loss 1.2348 (1.1346) acc 43.7500 (43.8425) (mean 41.4675 many 41.4675 med nan few nan) lr 6.5451e-03 eta 0:01:35
epoch [5/10] batch [30/46] time 0.304 (0.369) data 0.001 (0.069) loss 1.0629 (1.1253) acc 54.6875 (45.6740) (mean 53.6034 many 53.6034 med nan few nan) lr 6.5451e-03 eta 0:01:30
epoch [5/10] batch [40/46] time 0.304 (0.366) data 0.000 (0.066) loss 1.0938 (1.1158) acc 50.0000 (43.6301) (mean 55.0350 many 55.0350 med nan few nan) lr 6.5451e-03 eta 0:01:26
epoch [6/10] batch [10/46] time 0.304 (0.372) data 0.000 (0.072) loss 1.2010 (1.1406) acc 43.7500 (44.6446) (mean 39.2568 many 39.2568 med nan few nan) lr 5.0000e-03 eta 0:01:21
epoch [6/10] batch [20/46] time 0.304 (0.369) data 0.000 (0.069) loss 0.9938 (1.1196) acc 44.5312 (43.9314) (mean 46.2057 many 46.2057 med nan few nan) lr 5.0000e-03 eta 0:01:17
epoch [6/10] batch [30/46] time 0.305 (0.366) data 0.001 (0.067) loss 1.0829 (1.0977) acc 40.6250 (44.4135) (mean 44.1825 many 44.1825 med nan few nan) lr 5.0000e-03 eta 0:01:13
epoch [6/10] batch [40/46] time 0.302 (0.364) data 0.000 (0.064) loss 1.0980 (1.1014) acc 42.1875 (45.2509) (mean 47.5797 many 47.5797 med nan few nan) lr 5.0000e-03 eta 0:01:09
epoch [7/10] batch [10/46] time 0.305 (0.369) data 0.000 (0.070) loss 1.1025 (1.0841) acc 42.1875 (46.4721) (mean 45.7235 many 45.7235 med nan few nan) lr 3.4549e-03 eta 0:01:04
epoch [7/10] batch [20/46] time 0.304 (0.367) data 0.000 (0.067) loss 0.9865 (1.0900) acc 52.3438 (46.9367) (mean 59.9743 many 59.9743 med nan few nan) lr 3.4549e-03 eta 0:01:00
epoch [7/10] batch [30/46] time 0.303 (0.365) data 0.001 (0.065) loss 1.1027 (1.0751) acc 34.3750 (48.0429) (mean 48.8092 many 48.8092 med nan few nan) lr 3.4549e-03 eta 0:00:56
epoch [7/10] batch [40/46] time 0.303 (0.363) data 0.000 (0.063) loss 1.0021 (1.0627) acc 57.8125 (49.1883) (mean 49.4685 many 49.4685 med nan few nan) lr 3.4549e-03 eta 0:00:52
epoch [8/10] batch [10/46] time 0.305 (0.368) data 0.000 (0.069) loss 1.0569 (1.0760) acc 43.7500 (45.2251) (mean 49.6946 many 49.6946 med nan few nan) lr 2.0611e-03 eta 0:00:47
epoch [8/10] batch [20/46] time 0.305 (0.366) data 0.000 (0.067) loss 1.0716 (1.0519) acc 48.4375 (47.8881) (mean 45.7685 many 45.7685 med nan few nan) lr 2.0611e-03 eta 0:00:43
epoch [8/10] batch [30/46] time 0.305 (0.364) data 0.001 (0.065) loss 1.1182 (1.0684) acc 42.1875 (46.9669) (mean 49.6834 many 49.6834 med nan few nan) lr 2.0611e-03 eta 0:00:39
epoch [8/10] batch [40/46] time 0.304 (0.363) data 0.000 (0.063) loss 0.9889 (1.0586) acc 54.6875 (47.1643) (mean 56.7876 many 56.7876 med nan few nan) lr 2.0611e-03 eta 0:00:35
epoch [9/10] batch [10/46] time 0.304 (0.367) data 0.000 (0.068) loss 0.9485 (1.0134) acc 57.8125 (51.2516) (mean 56.7734 many 56.7734 med nan few nan) lr 9.5492e-04 eta 0:00:30
epoch [9/10] batch [20/46] time 0.305 (0.365) data 0.000 (0.066) loss 0.9621 (1.0352) acc 57.0312 (49.4613) (mean 53.8191 many 53.8191 med nan few nan) lr 9.5492e-04 eta 0:00:26
epoch [9/10] batch [30/46] time 0.306 (0.363) data 0.000 (0.064) loss 1.0352 (1.0620) acc 46.0938 (46.4803) (mean 42.4936 many 42.4936 med nan few nan) lr 9.5492e-04 eta 0:00:22
epoch [9/10] batch [40/46] time 0.303 (0.362) data 0.000 (0.063) loss 1.1105 (1.0510) acc 46.8750 (44.9239) (mean 47.1602 many 47.1602 med nan few nan) lr 9.5492e-04 eta 0:00:18
epoch [10/10] batch [10/46] time 0.304 (0.366) data 0.000 (0.067) loss 1.0091 (1.0099) acc 50.0000 (51.4256) (mean 54.6674 many 54.6674 med nan few nan) lr 2.4472e-04 eta 0:00:13
epoch [10/10] batch [20/46] time 0.304 (0.365) data 0.000 (0.066) loss 1.0175 (1.0375) acc 49.2188 (51.4801) (mean 49.8725 many 49.8725 med nan few nan) lr 2.4472e-04 eta 0:00:09
epoch [10/10] batch [30/46] time 0.304 (0.363) data 0.000 (0.064) loss 1.0342 (1.0277) acc 51.5625 (50.6155) (mean 57.0098 many 57.0098 med nan few nan) lr 2.4472e-04 eta 0:00:05
epoch [10/10] batch [40/46] time 0.303 (0.362) data 0.000 (0.063) loss 1.0428 (1.0208) acc 51.5625 (50.3960) (mean 48.8240 many 48.8240 med nan few nan) lr 2.4472e-04 eta 0:00:02
Finish training
Note that the printed training acc is not precise. To get precise training acc, use option ``test_train True``.
Time elapsed: 0:02:53
Checkpoint saved to ./output/kneeoa_clip_vit_b16_peft/checkpoint.pth.tar
Evaluate on the test set
=> result
* total: 1,656
* correct: 883
* accuracy: 53.3%
* error: 46.7%
* macro_f1: 53.1%
* class acc: [59.31 45.95 41.83 60.09 92.16]
* worst_case_acc: 41.8%
* hmean_acc: 55.5%
* gmean_acc: 57.6%
* many: 59.9%  med: nan%  few: nan%
* average: 59.9%
